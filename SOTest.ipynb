{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"stack_overflow_16k.tar.gz\", url, untar=True, cache_dir='.', cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 8000 files belonging to 4 classes.\nUsing 6400 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['csharp', 'java', 'javascript', 'python']\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'\"blank8 why is my solution faster than the neat solution? (hackerrank chocolate feast) edit: simplified my solution..edit: removed opinion based secondary question...background: atarted learning blank a week or two ago using hackerranks problems as exercises and stackoverflow search + google as my teacher, i\\'ve had some limited experience learning other languages...i did the exercise my own \"\"noobish learner way\"\" which i can\\'t help but feel is a \"\"botched job\"\" when i see \"\"neat &amp; short\"\" solutions...however, when submitting both solutions one after another a couple of times i found the \"\"neat\"\" solution was quite a bit slower. ..i vaguely remember something about % operations being costly, is mine faster because of no % operations or is there more to it than just that?..exercise: https://www.hackerrank.com/challenges/chocolate-feast..neat solution from discussion:..import blank.io.*;.import blank.util.*;..public class solution {.    static int cc; .    public static void main(string[] args) {.        scanner in = new scanner(system.in);.        int t,n,c,m,r;.            t = in.nextint();.            while(t--&gt;0){.             n = in.nextint();.            c = in.nextint();.             m = in.nextint();.                r=n/c;.                cc=r;..                    while(r&gt;=m){.                        cc=cc+r/m;.                        r=r%m+r/m;.                    }..                system.out.println(cc); .            }..    }.}...my solution:..import blank.io.*;.import blank.util.*;..public class solution {..    public static void main(string[] args) {..        scanner sc = new scanner(system.in);.        int t = integer.parseint(sc.nextline());    //t = number of test cases.        int[][] tc = readinput(sc, t);              //tc[t][0] = money. tc[t][1] = price. tc[t][2] = wrappers per free bar..        for (int i = 0; i&lt;t; i++){                  //loop for all test cases.            int choc = calcchoc(tc,i);              //work out how much choc can be bought.            system.out.println(choc);               //print result for the test case.        }.    }.    //calculate how much choc he can buy with m $ at p price with w wrappers needed for a free bar.    public static int calcchoc(int[][] tc,int i){..        int m = tc[i][0];       //money he has.        int p = tc[i][1];       //price of choc.        int w = tc[i][2];       //wrappers per free bar..        int bars = m/p;         //how many bars he can buy initially.        int wrappers = bars;    //each bar is a wrapper from initial purpose..        //loop to turn in all wrappers while it is possible to do so.        while (w&lt;=wrappers){..            int barsfromturnin = wrappers/w;                //bars from turning in current wrappers..            bars = bars + barsfromturnin;                   //new bar count.            wrappers = wrappers - (barsfromturnin * (w-1)); //wrapper count reduced by amount of wrappers turned in -1 wrapper per bar recieved from turn in...            if (w==1){ //break out of infinite loop when you get 1 bar for 1 wrapper!.                system.out.print(\"\"infinite bars, exiting infinite loop at bars = \"\");.                break;.            }.        }.        return bars;.    }.    //read input for each test case and make 2d array of the info.    public static int[][] readinput(scanner sc, int t){..        int[][] input = new int[t][3];..        for (int i = 0; i&lt;t; i++){.            string[] inputline = sc.nextline().split(\"\" \"\");..            input[i][0] = integer.parseint(inputline[0]);.            input[i][1] = integer.parseint(inputline[1]);.            input[i][2] = integer.parseint(inputline[2]);.        }.        return input;.    }.}\"\\n'\n1\n\nb'\"element.removeeventlistener(\\'mousedown\\', externalfunction, usecapture); is not working i need to first remove the event listener before dynamically adding more elements which also need the same event listener. i am using an external function name (not an anonymous function) and specifying the same usecapture value in both the add and remove. ..the function is nested within another function. &lt; suspected problem was the problem..you can see the problem by clicking the first \"\"add button\"\"  more than once. the first click adds one more button, the second click adds two more, the third click adds four more, etc. each click should only add one more. i guess the return value of removeeventlistener is always undefined so i can only tell that removal did not work from the duplicate events.....var app = function() {. console.log(\\'app\\');. . var setup = function() {.  console.log(\\'setup\\');. .  var addbutton = function(e) {.   console.log(e);.   var button = e.target;.   var newbutton = document.createelement(\\'button\\');.   newbutton.innertext = \\'add another button\\';.   button.parentnode.appendchild( newbutton );.   setup();.  }. .  var buttons = document.queryselectorall(\\'button\\');.  .  for(var i=0; i&lt;buttons.length; i++) {.   var button = buttons[i];.   button.removeeventlistener(\\'mousedown\\', addbutton, false);.   button.addeventlistener(\\'mousedown\\', addbutton, false);.  }.  . }. setup();.}.app();.&lt;div&gt;. &lt;button&gt;add button&lt;/button&gt;.&lt;/div&gt;\"\\n'\n2\n\nb'\"downloading a file using blank i have some code to download a text file from a website. when the requested file does not exist, my application downloads a text file which has html content. i need to filter this html content (should not download a text file with html content if the requested file does not exist) and need to download only text files which has the correct content. below is my code...string filepath = @\"\"c:textfiles\"\" + filename + string.format(\"\"{0:00000}\"\", i) + \"\".txt\"\";.directory.createdirectory(path.getdirectoryname(filepath));.//messagebox.show(filepath);..using (filestream download = new filestream(filepath, filemode.create)).{.    stream stream = clientx.getresponse().getresponsestream();.    while ((read = stream.read(buffer, 0, buffer.length)) != 0).    {..        download.write(buffer, 0, read);..    }.}...please advice\"\\n'\n0\n\nb'\"read/write windows registry on 64bit win 7 using blank with jna i\\'m trying to read/write windows registry on 64bit win7 using blank. ..firstly, i tried jdk blank.util.prefs.preferences and its reflection usage. that is a good solution but it only supports reading/writing reg_sz type (string) value. ..unfortunately, i need to read/write reg_binary, so give it up. ..secondly, i tried jni registry. reading is ok, but writing usually fails because writing hklm needs administrator rights. i don\\'t know how to get administrator rights in blank. ..finally, i tried jna (blank native access) an excellent project for working with native libraries and has support for the windows registry in the platform library (platform.jar) through advapi32util and advapi32. it\\'s very good and simple to use. and writing registry needs no administrator rights. ..but how can i read/write 32bit registry (under wow6432node node) in a 64bit jvm on win7? ..by default, 64bit nodes are read/written in 64bit jvms, and 32bit nodes in 32bit jvms. ..but in a 64bit jvm, i want to read/write 32bit nodes(for example, hklm-&gt;software-&gt;wow6432node-&gt;odbc). how can i do that?\"\\n'\n1\n\nb'\"why does my string.strip() function in blank fails to strip other characters along the same line? for instance, i have this:..stra = \"\"this is stringt exampltte....wow! !!0000000\"\".print stra.strip(\\'t\\')....  his is stringt exampltte....wow! !!0000000...why does the function only strip the first \\'t\\'?..in contrast, we have this:.    stra = \"\"000000this is stringt exampltte....wow! !!0000000\"\".    print stra.strip(\\'0\\')...  this is stringt exampltte....wow! !!...which strips all of the \\'0\\'. why would this remove all \\'0\\' in this case but not in the case of \\'t\\'?..third variation, we have this:..stra = \"\"000000this is stringt exampltte....wow! !!0000000\"\".print stra.strip(\\'this\\')....  000000this is stringt exampltte....wow! !!0000000...which removes nothing at all! not even in the words that have t,h,i,s in it. but if we were to remove the zeros from the start,..stra = \"\"this is stringt exampltte....wow! !!0000000\"\".print stra.strip(\\'this\\')....  is stringt exampltte....wow! !!0000000...it seems that the strip function in my blank 2.7.x only works for the first combination of characters separated by whitespace, or for only number strings! why is this so?\"\\n'\n3\n\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 0 =  csharp\nLabel 1 =  java\nLabel 2 =  javascript\nLabel 3 =  python\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 = \", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 = \", raw_train_ds.class_names[1])\n",
    "print(\"Label 2 = \", raw_train_ds.class_names[2])\n",
    "print(\"Label 3 = \", raw_train_ds.class_names[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 8000 files belonging to 4 classes.\nUsing 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory('test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=5000\n",
    "embedding_dim=128\n",
    "sequence_length=500\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "train_ds  = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 128)         640128    \n_________________________________________________________________\ndropout (Dropout)            (None, None, 128)         0         \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 128)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense (Dense)                (None, 4)                 516       \n=================================================================\nTotal params: 640,644\nTrainable params: 640,644\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features + 1, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.8963 - accuracy: 0.7286 - val_loss: 0.8644 - val_accuracy: 0.7381\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.8144 - accuracy: 0.7516 - val_loss: 0.7977 - val_accuracy: 0.7481\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.7486 - accuracy: 0.7719 - val_loss: 0.7440 - val_accuracy: 0.7600\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.6948 - accuracy: 0.7862 - val_loss: 0.7008 - val_accuracy: 0.7738\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 5s 25ms/step - loss: 0.6479 - accuracy: 0.7986 - val_loss: 0.6647 - val_accuracy: 0.7869\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 0.6063 - accuracy: 0.8145 - val_loss: 0.6346 - val_accuracy: 0.7944\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.5720 - accuracy: 0.8275 - val_loss: 0.6091 - val_accuracy: 0.7962\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.5395 - accuracy: 0.8363 - val_loss: 0.5875 - val_accuracy: 0.7987\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 0.5111 - accuracy: 0.8477 - val_loss: 0.5690 - val_accuracy: 0.8000\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 0.4858 - accuracy: 0.8558 - val_loss: 0.5528 - val_accuracy: 0.8087\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "validation_data=val_ds,\n",
    "epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "250/250 [==============================] - 1s 3ms/step - loss: 0.6100 - accuracy: 0.7795\n",
      "Loss:  0.6100177764892578\n",
      "Accuracy:  0.7795000076293945\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}